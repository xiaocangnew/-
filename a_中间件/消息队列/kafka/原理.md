### 基础架构
一个典型的Kafka体系架构包括:
* 以及一个Zookeeper集群。
* 若干broker(Kafka支持水平扩展，一般broker数量越多，集群吞吐率越高， broker注册到zookeepers上)
* 若干Producer,
* 若干Consumer(producer和consumer都是和broker通信的，老版本consumer和zookeeper通信)

### topic
[topic 创建过程](https://www.cnblogs.com/huxi2b/p/5923252.html)

### 消费者-协调器
- 0.10版本引入了协调器概念，包括两个(存放在broker中，和zk没关系)：
- groupCoordinator
   用于管理消费者组和该消费者组下每个消费者的消费偏移量。每个kafkaServer(broker)启动时都会启动一个协调器；
- consumerCoordinator
   负责同一个消费者组下各消费者与 GroupCoordinator进行通信。每个consumer启动时都会启动一个协调器；
   
- groupCoordinator与consumerCoordinator通信过程
   - 1. 心跳(从消费者宕机到会话过期是有一定时间的，这段时间内该消费者的分区都不能进行消息消费；)
          优雅关闭时消费者会发送离开的消息到组协调者，这样组协调者可以立即进行重平衡而不需要等待会话过期。
   - 2. 分区再均衡ConsumerRebalanceListener(再均衡过程中不提供consume服务)
          当发生以下情况，会进行再均衡：
          1.新加入consumer；2.新离开consumer； 3.topic新增加分区时；
   - 3. consumerLeader分配分区策略(此时groupCoordinator只存储和中间转发人，各个consumer之间不通信)
         1. 当消费者要加入群组时，它会向群组协调器发送一个 JoinGroup 请求。第一个加入群组的消费者将成为leader消费者。
             leader消费者从组协调器那里获得群组的成员列表，并负责给每一个消费者分配分区。 
         2. 每个消费者的消费者协调器在向组协调器请求加入组时，都会把自己支持的分区分配策略报告给组协调器(轮询或者是按跨度分配或者其他)，
             组协调器选出该消费组下所有消费者都支持的的分区分配策略发送给leader消费者，leader消费者根据这个分区分配策略进行分配。
         3. leader消费者把分配情况列表发送给组协调器，组协调器把这些信息发送给所有消费者。
            每个消费者只能看到自己的分配信息，只有leader消费者知道群组里所有消费者的分配信息。这个过程会在每次再均衡时重复发生。
-  consumerLeader支持的分区分配策略
     1.轮询： 把消费者组消费的所有topic 拿出来，统一排序，轮询给所有consumer。 缺点：在多topic下，导一个topic被好几个consumer消费；
     2.range策略(默认)： 把消费同一个topic的所有consumer排序，尽量均分；如果不平均的话，排在前面的多，0-2parition归consumer1，3-4partition归consumer2
     
## 消息存储
- partition
  说明:分区，对应目录中的文件夹： topic-0 、 topic-1...
- LogSegment
  说明: 每个分区被分为多个片段，是一个逻辑概念, 对应三个文件  .log + .index + .timeindex
  文件说明：
  - .index文件
    使用稀疏索引的方式建立(每隔一定的字节数建立了一条索引)，格式内容：offset: 22372444 position: 16365
    减少索引文件大小，使用mmap技术直接映射内存，来加快查找。
     0. 使用参数 “index.interval.bytes”设置索引的跨度
     1. 二分查找法，找到相应的稀疏索引，然后跟进index上的偏移量，找到log文件的位置，
     2. 顺序遍历log文件找到相应的数据；
  - .timeindex文件
  
-message格式
  说明：固定消息头+可变消息体
  消息体中几个主要字段：
   - offset 消息偏移量
   - message size 消息总长度
   - CRC32 编码校验和
   - key 消息key的时实际数据
   - key length 消息key长度
   - valuesize 消息的实际数据长度
   - playload 消息的实际数据
    
- 顺序写入磁盘
  通过追加写的方式来尽可能的将随机I/O转换为顺序I/O，以此来降低寻址时间和旋转延时，从而最大限度的提高IOPS。

- 旧日志删除策略
   - 基于时间
      Log.retention.hours=168(默认保留7天)
   - 基于大小
      Log.retention.bytes=1073741824(默认1G)
      读取特定消息的时间复杂度为O(1)，删除数据不会对效率产生影响

#### 名词解释
- Controller：集群管理控制器（本身也是个broker）
- ISR（In Sync Replica）：副本同步组，表示基本跟上leader的replica
- LEO（Log End Offset）：每个partition的log最后一条Message的位置。
- HW（High Watermark）：高水位，取ISR中最小的LEO作为HW，消费者最多只能消费到HW所在的位置。

* controller
  - 功能
    增加删除topic，更新分区副本数量，选举partition分区leader，集群broker增加和宕机后的调整，当然还有自身的选举controller leader功能
  - controller失败 
   幸存的所有broker都会尝试在Zookeeper中创建/controller->{this broker id}，如果创建成功（只可能有一个创建成功），则该broker会成为controller，若创建不成功，则该broker会等待新controller的命令。
  - 作用
   降低了zookeeper的负载。要是所有的broker都监视/brokers/ids那么一旦集群成员发生变化,那么zk得挨个提醒broker回调.但是现在只用提醒leader就行了,
   唯一让zk很辛苦的情况就是leader崩溃.不过这种情况比较少

### ISR复制(使用pull主动拉， 个人感觉使用ack=-1时是主动推送，还没找到答案。)
- 同步复制
   request.required.acks
   发送消息要求ACK，全部ISR都接收到之后消息才会被commit
   不丢消息、吞吐率受影响
- 异步复制
   不要求ACK
   ISR定期向leader批量请求数据，进行同步
   当所有ISR都落后时，如果leader挂掉，则丢数据
   平衡了性能-数据不丢失
   
- 主动拉的参数设置
  - replica.lag.max.messages 落后的消息个数(0.10.x版本移除，因为瞬时大量消息会导致ISR中没有问题的replica也被移除了)
  - replica.lag.time.max.ms 多长时间没有发送FetchQuest请求拉去leader数据
