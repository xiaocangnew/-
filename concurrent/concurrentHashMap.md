### [问题](https://blog.csdn.net/qq_41737716/article/details/90549847)
1.ConcurrentHashMap是怎么做到线程安全的？
   - 使用volatile保证当Node中的值变化时对于其他线程是可见的
   - 使用table数组的头结点作为synchronized的锁来保证写操作的安全
   - 当头结点为null时，使用CAS操作来保证数据能正确的写入。
2.get方法如何线程安全地获取key、value？
3.put方法如何线程安全地设置key、value？
4.size方法如果线程安全地获取容器容量？
5.底层数据结构扩容时如果保证线程安全？
6.初始化数据结构时如果保证线程安全？
7.ConcurrentHashMap并发效率是如何提高的？
8.和加锁相比较，为什么它比HashTable效率高？

### 红黑树
在链表长度超过8，Node数组超过64时会将链表结构转换为红黑树，Node对象：

### jdk1.7 <-> jdk1.8
- 结构上的变化:
1.取消原先的Segment设计，取而代之的是使用与HashMap同样的数据结构，但其内部仍然有Segment定义，但仅仅是为了保证序列化时的兼容性而已，不再有任何结构上的用处
2.哈希表+红黑树,并且引入了懒加载机制。（JDK1.7一上来就初始化，JDK1.8 在第一次put时才初始化）

- 线程安全:
1.锁粒度更细:由原来的锁Segment一片区域到锁桶的头结点
2.由原先的ReentrantLock替换为Sychronized+CAS
3.现版本的sychronized已经经过不断优化，性能上与ReentrantLock基本没有差异,
  相对于ReentrantLock，使用Sychronized可以节省大量内存空间（原来ReentrantLock下的segment都得加入同步队列，都得继承AQS下的Node，而synchronized只是锁住头结点，头结点下边的节点都不会加入同步队列里，所以 节省了空间），这是非常大的优势所在。


### 底层数据结构：
- jdk1.7
    - 其本质是一个segment[]数组,分段锁就是对segment进行，每个segment维护一个HashEntry<K,V>[]数组，扩容时只能扩单个segment下的容，整体不能扩容，在new时就订好了；
    - 这个链表是final类型不可更改。1.新增数据只能加到链表头。2.删除时只能复制删除节点前面的部分，并使得next指向删除节点的下一个节点。
- jdk1.8
    与hashMap一样

### 初始化数据结构时的线程安全
- 在jdk1.7中
1.新建时就初始化；
- 在JDK1.8中
1.初始化ConcurrentHashMap的时候这个Node[]数组是还未初始化的，会等到第一次put方法调用时才初始化：
2.如果多个线程同时调用initTable初始化Node数组会有并发问题, 初始化数组使用了乐观锁CAS操作来决定到底是哪个线程有资格进行初始化，其他线程均只能等待。
    - 用到的并发技巧：
      1.volatile变量（sizeCtl）：它是一个标记位，用来告诉其他线程这个坑位有没有人在，其线程间的可见性由volatile保证。
      2.CAS操作：CAS操作保证了设置sizeCtl标记位的原子性，保证了只有一个线程能设置成功

### put操作的线程安全
- jdk1.8
   减小锁粒度：将Node链表的头节点作为锁，若在默认大小16情况下，将有16把锁，大大减小了锁竞争，
   将串行的部分最大化缩小，在理想情况下线程的put操作都为并行操作。同时直接synchronized锁住头节点，保证了线程安全

### get操作 
- jdk1.7 不需要进行加锁
   通过Unsafe.getObjectVolatile()方法提供的原子读语义，来获得Segment以及对应的链表，然后对链表遍历判断是否存在key相同的节点以及获得该节点的value。
   由于遍历过程中其他线程可能对链表结构做了调整(调整时，由于final,所以修改会复制原来的数组，同时并行两个数组)，因此get和containsKey返回的可能是过时的数据，这一点是ConcurrentHashMap在弱一致性上的体现。如果要求强一致性，那么必须整个map加锁
- jdk1.8
   对于get操作，其实没有线程安全的问题，只有可见性的问题，只需要确保get的数据是线程之间可见的即可：
   使用了tabAt方法Unsafe类volatile的方式去获取Node数组中的Node，保证获得到的Node是最新的
   
### 扩容时的线程安全 
- [扩容操作](https://blog.csdn.net/ZOKEKAI/article/details/90051567)
    ConcurrentHashMap支持多线程并发扩容，在扩容过程中同时支持get查数据，若有线程put数据，还会帮助一起扩容，这种无阻塞算法，将并行最大化的设计
- 基础概念
   - 变量:sizeCtl
      1)map未初始化时记录的是初始容量大小
      2)在初始化过程中将sizeCtl= -1,其他线程发现该值为 -1 时会让出CPU资源以便初始化操作尽快完成
      3)正常后sizeCtl 用于记录触发集合扩容的极限值
      4)在扩容过程中记录当前扩容的并发线程数
   - 变量transferIndex
       数组上hash桶的迁移工作已经'分配到'的位置(从右到左分配)
   - 线程内迁移
      1)stride：当前线程需要迁移hash桶的个数
      2)i:当前线程迁移任务开始下标； bond：结束下标， 从右到左迁移；
      3）每个线程承担不小于 16 个槽中的元素的扩容，然后从右向左划分16个槽给当前线程去迁移，
      每当开始迁移一个槽中的元素的时候，线程会锁住当前槽中列表的头元素
   - forwardingNode
      1）标识该hash桶的数据迁移已经完成
      2）转发，遇到get操作时转发到扩容后的新数组上
   - 迁移过程中的ln（低位Node）、hn（高位Node）
      1)在put值的时候，首先会计算hash值，再散列到指定的Node数组下标中(1.得到key的hashCode; 2.使用(n - 1) & hash 运算，定位Node数组中下标值)
      2)低位链表放入原下标处，而高位链表则需要加上原Node数组长度

- 常见问题
   - 扩容如何保证线程安全
      
      
   - 触发扩容条件：
      - 在put值时，发现Node为占位Node（fwd）时，会协助扩容
      - 在新增节点后，检测到链表长度大于8时，同时数组长度小于64。
      - 超过总容量后扩容
   - 扩容时put
      - 插入的位置扩容线程还未迁移到，直接插入
      - 只要，当迁移到该插入的位置时，就会阻塞等待插入操作完成再继续迁移 
   - 扩容时get
      - 扩容过程期间形成的hn和ln链是使用的复制引用的方式，原来 hash 桶上的链表并没有受到影响
      - 从迁移开始到迁移结束这段时间都是可以正常访问原数组hash桶上面的链表
      - 迁移结束后放置上fwd，往后的访问请求就直接转发到扩容后的数组去了
   - 扩容完成后为什么要再检查一遍
      - 因为扩容是分段进行的，当前线程扩容完，其他线程可能还没有扩容完成，需要再检查一次，而且会帮忙一起扩容


### size()的线程安全
- 基于整个ConcurrentHashMap操作,原理：
    - 首先不加锁循环执行以下操作：循环所有的Segment(通过Unsafe的getObjectVolatile()以保证原子读语义），获得对应的值以及所有Segment的modcount之和。
      如果连续两次所有Segment的modcount和相等，则过程中没有发生其他线程修改ConcurrentHashMap的情况，返回获得的值。
      当循环次数超过预定义的值时，这时需要对所有的Segment依次进行加锁，获取返回值后再依次解锁。值得注意的是，加锁过程中要强制创建所有的Segment，否则容易出现其他线程创建Segment并进行put，remove等操作。

### jdk1.7 有并发度概念，  1.8 没有
- 就是ConcurrentHashMap中的分段锁个数，即Segment[]的数组长度。
   ConcurrentHashMap默认的并发度为16。当用户设置并发度时，ConcurrentHashMap会使用大于等于该值的最小2幂指数作为实际并发度（假如用户设置并发度为17，实际并发度则为32）。
- 并发度设置的过小，会带来严重的锁竞争问题；
- 并发度设置的过大，原本位于同一个Segment内的访问会扩散到不同的Segment中，CPU cache命中率会下降，从而引起程序性能下降。

### putIfAbent() 
- ConcurrentHashMap本身是一个线程安全的容器，putIfAbent()也没有问题。
- 但是其不适合保存创建计算机资源的场景。因为计算机资源诸如：线程池、IO等都是有限的，如果还不涉及到自动回收的话就更宝贵了。
putIfAbent()每次都会创建成功，但不一定被存放到Map中，就不会被使用到，就造成了浪费。