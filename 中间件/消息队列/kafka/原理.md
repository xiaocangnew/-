### 基础架构
一个典型的Kafka体系架构包括:
* 以及一个Zookeeper集群。
* 若干broker(Kafka支持水平扩展，一般broker数量越多，集群吞吐率越高， broker注册到zookeepers上)
* 若干Producer,
* 若干Consumer(producer和consumer都是和broker通信的，老版本consumer和zookeeper通信)

### 基础思路梳理
1. Kafka使用一个选举出来的broker controller来监听zookeeper，其他broker再去和controller broker通信
2. broker controller负责管理：
    增加删除topic，更新分区副本数量，选举partition分区leader，集群broker增加和宕机后的调整
3. 这样每个broker都知道所有情况了
4. producer发消息到broker的具体分区上。发送时与broker通信获取元数据，知道分区情况；
5. consumer与broker通过协调器进行通信。进行消费。

### zookeeper在kafka中的作用
1. 使用zookeeper的配置保存机制
       保存kafka元数据信息。有个brokers节点，保存了brokers信息，topic信息，分区信息等元数据。
2. zookeeper本身的选举机制
      选举broker controller。
      新版的 Kafka使用一个选举出来的controller来监听zookeeper，controller和其他node通信，
      这么做的目的是为了减少zookeeper的压力。
* 3. zookeeper本身有watcher机制。一旦zookeeper发生变化，客户端能及时感知并作出相应调整。
   这样就保证了添加或去除broker时，各broker间仍能自动实现负载均衡。  
   这里的客户端指的是Kafka的消息生产端(Producer)和消息消费端(Consumer)
* 4. zookeeper本身的注册机制。
   Consumer端使用zookeeper用来注册consumer信息,其中包括consumer消费的partition列表等,
   同时也用来发现broker列表,并和partitionleader建立socket连接,并获取消息.
* 5. kafka里面有broker/producer/consumer三个角色。
    Zookeeper和Brokers建立关系以实现负载均衡
    Zookeeper和Producer没有建立关系(消息发往broker)，Producer是瞬态的，可以发送后关闭，无需直接等待
    Zookeeper和consumer没有建立联系(consumer从broker获取消息)。
      0.8以前的kafka，消费的进度(offset)是写在zk中的，所以consumer需要知道zk的地址
      0.9 的时候整体大改了一次，brokers接管了消费者组和offset信息，consumer不再需要和zookeeper 通信了
  

### topic
[topic 创建过程](https://www.cnblogs.com/huxi2b/p/5923252.html)

### producer动态感知topic分区变化
1. producer向broker发送消息时，需要指定分区情况。
2. 如果在Kafka Producer往Kafka的Broker发送消息的时候用户通过命令修改了改主题的分区数，
Kafka Producer能动态感知吗？答案是可以的。
3. 那是立刻就感知到吗？不是，是过一定的时间（topic.metadata.refresh.interval.ms参数决定）
才知道分区数改变的

### 消费者-协调器
- 0.10版本引入了协调器概念，包括两个(存放在broker中，和zk没关系)：
- groupCoordinator
   用于管理消费者组和该消费者组下每个消费者的消费偏移量。每个kafkaServer(broker)启动时都会启动一个协调器；
- consumerCoordinator
   负责同一个消费者组下各消费者与 GroupCoordinator进行通信。每个consumer启动时都会启动一个协调器；
   
- groupCoordinator与consumerCoordinator通信过程
   - 1. 心跳(从消费者宕机到会话过期是有一定时间的，这段时间内该消费者的分区都不能进行消息消费；)
          优雅关闭时消费者会发送离开的消息到组协调者，这样组协调者可以立即进行重平衡而不需要等待会话过期。
   - 2. 分区再均衡ConsumerRebalanceListener(再均衡过程中不提供consume服务)
          当发生以下情况，会进行再均衡：
          1.新加入consumer；2.新离开consumer； 3.topic新增加分区时；
   - 3. consumerLeader分配分区策略(此时groupCoordinator只存储和中间转发人，各个consumer之间不通信)
         1. 当消费者要加入群组时，它会向群组协调器发送一个 JoinGroup 请求。第一个加入群组的消费者将成为leader消费者。
             leader消费者从组协调器那里获得群组的成员列表，并负责给每一个消费者分配分区。 
         2. 每个消费者的消费者协调器在向组协调器请求加入组时，都会把自己支持的分区分配策略报告给组协调器(轮询或者是按跨度分配或者其他)，
             组协调器选出该消费组下所有消费者都支持的的分区分配策略发送给leader消费者，leader消费者根据这个分区分配策略进行分配。
         3. leader消费者把分配情况列表发送给组协调器，组协调器把这些信息发送给所有消费者。
            每个消费者只能看到自己的分配信息，只有leader消费者知道群组里所有消费者的分配信息。这个过程会在每次再均衡时重复发生。
-  consumerLeader支持的分区分配策略
     1.轮询： 把消费者组消费的所有topic 拿出来，统一排序，轮询给所有consumer。 缺点：在多topic下，导一个topic被好几个consumer消费；
     2.range策略(默认)： 把消费同一个topic的所有consumer排序，尽量均分；如果不平均的话，排在前面的多，0-2parition归consumer1，3-4partition归consumer2
     
## 消息存储
- partition
  说明:分区，对应目录中的文件夹： topic-0 、 topic-1...
- LogSegment
  说明: 每个分区被分为多个片段，是一个逻辑概念, 对应三个文件  .log + .index + .timeindex
  文件说明：
  - .index文件
    使用稀疏索引的方式建立(每隔一定的字节数建立了一条索引)，格式内容：offset: 22372444 position: 16365
    减少索引文件大小，使用mmap技术直接映射内存，来加快查找。
     0. 使用参数 “index.interval.bytes”设置索引的跨度
     1. 二分查找法，找到相应的稀疏索引，然后跟进index上的偏移量，找到log文件的位置，
     2. 顺序遍历log文件找到相应的数据；
  - .timeindex文件
  
-message格式
  说明：固定消息头+可变消息体
  消息体中几个主要字段：
   - offset 消息偏移量
   - message size 消息总长度
   - CRC32 编码校验和
   - key 消息key的时实际数据
   - key length 消息key长度
   - valuesize 消息的实际数据长度
   - playload 消息的实际数据
    
- 顺序写入磁盘
  通过追加写的方式来尽可能的将随机I/O转换为顺序I/O，以此来降低寻址时间和旋转延时，从而最大限度的提高IOPS。

- 旧日志删除策略
   - 基于时间
      Log.retention.hours=168(默认保留7天)
   - 基于大小
      Log.retention.bytes=1073741824(默认1G)
      读取特定消息的时间复杂度为O(1)，删除数据不会对效率产生影响
      
   

#### 名词解释
- Controller：集群管理控制器（本身也是个broker）
- ISR（In Sync Replica）：副本同步组，表示基本跟上leader的replica
- LEO（Log End Offset）：每个partition的log最后一条Message的位置。
- HW（High Watermark）：高水位，取ISR中最小的LEO作为HW，消费者最多只能消费到HW所在的位置。

* controller
  - 功能
    增加删除topic，更新分区副本数量，选举partition分区leader，集群broker增加和宕机后的调整，当然还有自身的选举controller leader功能
  - controller失败 
   幸存的所有broker都会尝试在Zookeeper中创建/controller->{this broker id}，如果创建成功（只可能有一个创建成功），则该broker会成为controller，若创建不成功，则该broker会等待新controller的命令。
  - 作用
   降低了zookeeper的负载。要是所有的broker都监视/brokers/ids那么一旦集群成员发生变化,那么zk得挨个提醒broker回调.但是现在只用提醒leader就行了,
   唯一让zk很辛苦的情况就是leader崩溃.不过这种情况比较少

### ISR复制(使用pull主动拉， 个人感觉使用ack=-1时是主动推送，还没找到答案。)
- 同步复制
   request.required.acks
   发送消息要求ACK，全部ISR都接收到之后消息才会被commit
   不丢消息、吞吐率受影响
- 异步复制
   不要求ACK
   ISR定期向leader批量请求数据，进行同步
   当所有ISR都落后时，如果leader挂掉，则丢数据
   平衡了性能-数据不丢失
   
- 主动拉的参数设置
  - replica.lag.max.messages 落后的消息个数(0.10.x版本移除，因为瞬时大量消息会导致ISR中没有问题的replica也被移除了)
  - replica.lag.time.max.ms 多长时间没有发送FetchQuest请求拉去leader数据
